{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"k80kpAq5W9I7"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wit237RobN_0"},"outputs":[],"source":["# Check cuda version\n","!nvcc --version\n","\n","# Install required packages\n","%tensorflow_version 2.x\n","!pip install gast==0.2.2\n","!pip uninstall -y tensorflow\n","!pip install tensorflow-gpu==1.14.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ER2XQ8LXTPC"},"outputs":[],"source":["%cd gdrive/MyDrive/Mini-Project/snip"]},{"cell_type":"code","source":["# Define a dummy class\n","class Args:\n","    def __init__(self):\n","        self.arch = 'lenet300'\n","        self.aug_kinds = ['fliplr', 'translate_px']\n","        self.batch_size = 500\n","        self.check_interval = 100\n","        self.datasource = 'mnist'\n","        self.decay_boundaries = [5000, 10000, 15000, 20000, 25000]\n","        self.decay_values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n","        self.initializer_b_ap = 'zeros'\n","        self.initializer_b_bp = 'zeros'\n","        self.initializer_w_ap = 'vs'\n","        self.initializer_w_bp = 'vs'\n","        self.logdir = './results'\n","        self.lr = 0.1\n","        self.lr_decay_type = 'piecewise'\n","        self.optimizer = 'momentum'\n","        self.path_assess = self.logdir + '/assess'\n","        self.path_data = '/content/gdrive/MyDrive/Mini-Project'\n","        self.path_model = self.logdir + '/model'\n","        self.path_summary = self.logdir + '/summary'\n","        self.save_interval = 100\n","        self.target_sparsity = 0.9\n","        self.train_iterations = 30000\n","        self.is_sample = False\n","        self.sample_class = 0"],"metadata":{"id":"PIuLNDM1-ET2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hm364m8YNrhR"},"source":["# Comparisons under Varied Sparsity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScHLRkUkN299"},"outputs":[],"source":["import os\n","import sys\n","import argparse\n","import tensorflow as tf\n","\n","from dataset import Dataset\n","from model import Model\n","import prune\n","import train\n","import test\n","\n","\n","# Set arguments\n","args = Args()\n","args.arch = 'lenet300' # lenet300 or lenet5\n","args.batch_size = 500\n","args.datasource = 'mnist' # mnist or kmnist\n","args.decay_boundaries = [5000, 10000, 15000, 20000, 25000]\n","if args.arch == 'lenet300':\n","    args.decay_values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n","elif args.arch == 'lenet5':\n","    args.decay_values = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n","args.target_sparsity = 0\n","args.train_iterations = 30000\n","\n","\n","for iter in range(0, 10):\n","    print(\"|--------- Iteration {} ---------|\".format(iter))\n","    \n","    # Set arguments\n","    args.logdir = './VariedSparsity/iter' + str(iter)\n","    args.path_assess = args.logdir + '/assess'\n","    args.path_model = args.logdir + '/model'\n","    args.path_summary = args.logdir + '/summary'\n","\n","    # Dataset\n","    dataset = Dataset(**vars(args))\n","\n","    # Reset the default graph and set a graph-level seed\n","    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","    tf.reset_default_graph()\n","    tf.set_random_seed(iter)\n","\n","    # Model\n","    model = Model(num_classes=dataset.num_classes, **vars(args))\n","    model.construct_model()\n","\n","    # Session\n","    sess = tf.InteractiveSession()\n","    tf.global_variables_initializer().run()\n","    tf.local_variables_initializer().run()\n","\n","    # Prune\n","    prune.prune(args, model, sess, dataset)\n","\n","    # Train and test\n","    train.train(args, model, sess, dataset)\n","    test.test(args, model, sess, dataset)\n","\n","    sess.close()"]},{"cell_type":"markdown","metadata":{"id":"e6Db_WyATdED"},"source":["# Comparisons with Multiple Network Architectures"]},{"cell_type":"code","source":["import os\n","import sys\n","import argparse\n","import tensorflow as tf\n","\n","from dataset import Dataset\n","from model import Model\n","import prune\n","import train\n","import test\n","\n","\n","# Set arguments\n","args = Args()\n","# Set a network architecture\n","# Convolutional: alexnet-v1, alexnet-v2, vgg-c, vgg-d, vgg-like\n","# Residual: resnet-18, resnet-34\n","# https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n","# Squeeze: squeezenet-vanilla, squeezenet-bypass\n","# https://arxiv.org/pdf/1602.07360.pdf?ref=https://githubhelp.com\n","# Inception: googlenet\n","# https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\n","# Dense: densenet-121, densenet-169, densenet-201, densenet-264\n","# https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\n","args.arch = 'resnet-18'\n","args.batch_size = 512\n","args.datasource = 'cifar-10'\n","args.decay_boundaries = [7500, 15000, 22500, 30000]\n","args.decay_values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n","args.target_sparsity = 0.95\n","args.train_iterations = 37500\n","\n","\n","for iter in range(0, 10):\n","    print(\"|--------- Iteration {} ---------|\".format(iter))\n","\n","    # Set arguments\n","    args.logdir = './NetworkArchitectures/iter' + str(iter)\n","    args.path_assess = args.logdir + '/assess'\n","    args.path_model = args.logdir + '/model'\n","    args.path_summary = args.logdir + '/summary'\n","\n","    # Dataset\n","    dataset = Dataset(**vars(args))\n","\n","    # Reset the default graph and set a graph-level seed\n","    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","    tf.reset_default_graph()\n","    tf.set_random_seed(iter)\n","\n","    # Model\n","    model = Model(num_classes=dataset.num_classes, **vars(args))\n","    model.construct_model()\n","\n","    # Session\n","    sess = tf.InteractiveSession()\n","    tf.global_variables_initializer().run()\n","    tf.local_variables_initializer().run()\n","\n","    # Prune\n","    prune.prune(args, model, sess, dataset)\n","\n","    # Train and test\n","    train.train(args, model, sess, dataset)\n","    test.test(args, model, sess, dataset)\n","\n","    sess.close()"],"metadata":{"id":"OWyDm3dnCeHi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Kp-gXEcTR0r"},"source":["# Understanding Which Connections Are Pruned"]},{"cell_type":"markdown","source":["## The Visualizations"],"metadata":{"id":"AJgx2P07L2sU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQzIafc8TP2A"},"outputs":[],"source":["import os\n","import sys\n","import argparse\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","\n","from dataset import Dataset\n","from model import Model\n","\n","# Set arguments\n","args = Args()\n","args.arch = 'lenet300'\n","args.datasource = 'mnist' # mnist or kmnist\n","\n","\n","images = np.zeros((9, 10, 28, 28))\n","\n","\n","for sample_class in range(0, 10):\n","    print(\"|--------- Sample class {} ---------|\".format(sample_class))\n","    for target_sparsity in range(1, 10):\n","        print(\"|--------- Target sparsity {} ---------|\".format(target_sparsity * 0.1))\n","\n","        # Set arguments\n","        args.is_sample = True\n","        args.sample_class = sample_class\n","        args.target_sparsity = target_sparsity * 0.1\n","\n","        # Dataset\n","        sampleset = Dataset(**vars(args))\n","\n","        # Reset the default graph and set a graph-level seed\n","        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","        tf.reset_default_graph()\n","        tf.set_random_seed(target_sparsity)\n","\n","        # Model\n","        model = Model(num_classes=dataset.num_classes, **vars(args))\n","        model.construct_model()\n","\n","        # Session\n","        sess = tf.InteractiveSession()\n","        tf.global_variables_initializer().run()\n","        tf.local_variables_initializer().run()\n","\n","        # Prune\n","        print('|========= START PRUNING =========|')\n","        t_start = time.time()\n","        batch = sampleset.get_next_batch('train', 100)\n","        feed_dict = {}\n","        feed_dict.update({model.inputs[key]: batch[key] for key in ['input', 'label']})\n","        feed_dict.update({model.compress: True, model.is_train: False, model.pruned: False})\n","        result = sess.run([model.outputs, model.sparsity, model.mask['w1']], feed_dict)\n","        print('Pruning: {:.3f} global sparsity (t:{:.1f})'.format(result[1], time.time() - t_start))\n","\n","        # Average and reshape the indicator matrix c_1\n","        w1_mask = np.array(result[-1])\n","        images[target_sparsity-1, sample_class] = np.reshape(np.mean(w1_mask, axis=1), (28, 28))\n","\n","        sess.close()"]},{"cell_type":"code","source":["# Plot the visualizations of pruned parameters\n","fig, ax = plt.subplots(nrows=9, ncols=10, figsize=[28, 28], squeeze=True)\n","fig = plt.subplots_adjust(wspace=0.1, hspace=0.01)\n","\n","for i, axes in enumerate(ax.flat):\n","    ir = i // 10\n","    ic = i % 10\n","    axes.imshow(images[ir, ic] * 255, cmap='gray', vmin=0, vmax=255)\n","    axes.axis('off')"],"metadata":{"id":"jtxthebNryvJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The Ratios"],"metadata":{"id":"R56h98xHLtu8"}},{"cell_type":"code","source":["import os\n","import sys\n","import argparse\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","\n","from dataset import Dataset\n","from model import Model\n","\n","\n","# Set arguments\n","args = Args()\n","args.arch = 'lenet300'\n","args.datasource = 'kmnist' # mnist or kmnist\n","\n","\n","ratios = np.zeros((10, 6))\n","\n","\n","for target_sparsity in range(0, 10):\n","    print(\"|--------- Target sparsity {} ---------|\".format(target_sparsity * 0.1))\n","\n","    # Set arguments\n","    args.target_sparsity = target_sparsity * 0.1\n","\n","    # Dataset\n","    sampleset = Dataset(**vars(args))\n","\n","    # Reset the default graph and set a graph-level seed\n","    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","    tf.reset_default_graph()\n","    tf.set_random_seed(target_sparsity)\n","\n","    # Model\n","    model = Model(num_classes=dataset.num_classes, **vars(args))\n","    model.construct_model()\n","\n","    # Session\n","    sess = tf.InteractiveSession()\n","    tf.global_variables_initializer().run()\n","    tf.local_variables_initializer().run()\n","\n","    # Prune\n","    print('|========= START PRUNING =========|')\n","    t_start = time.time()\n","    batch = sampleset.get_next_batch('train', 100)\n","    feed_dict = {}\n","    feed_dict.update({model.inputs[key]: batch[key] for key in ['input', 'label']})\n","    feed_dict.update({model.compress: True, model.is_train: False, model.pruned: False})\n","    result = sess.run([model.outputs, model.sparsity, model.mask], feed_dict)\n","    print('Pruning: {:.3f} global sparsity (t:{:.1f})'.format(result[1], time.time() - t_start))\n","\n","    # Compute the ratios\n","    masks = result[-1]\n","    count = 0\n","    for key in masks.keys():\n","        if 'w' in key:\n","            mask = np.array(masks[key])\n","            ratios[target_sparsity, count] = np.count_nonzero(mask) / np.size(mask)\n","            ratios[target_sparsity, count+1] = 1 - ratios[target_sparsity, count]\n","            count += 2\n","\n","    sess.close()"],"metadata":{"id":"co7z_Lm-LvwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index in range(0, 10):\n","    print('{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f};'.format(\n","        ratios[index, 1], ratios[index, 0], ratios[index, 3], \n","        ratios[index, 2], ratios[index, 5], ratios[index, 4]))"],"metadata":{"id":"Wg4B6pUhPfeg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Effects of Batch Sizes"],"metadata":{"id":"GCtvUq0HxnpR"}},{"cell_type":"markdown","source":["## Batch Size for Pruning"],"metadata":{"id":"8jmuE-qqMHP8"}},{"cell_type":"code","source":["import os\n","import sys\n","import argparse\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","\n","from dataset import Dataset\n","from model import Model\n","import train\n","import test\n","\n","\n","# Set arguments\n","args = Args()\n","args.arch = 'lenet300' # lenet300 or lenet5\n","args.batch_size = 500\n","args.datasource = 'mnist' # mnist or kmnist\n","args.decay_boundaries = [5000, 10000, 15000, 20000, 25000]\n","args.decay_values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n","args.target_sparsity = 0.9\n","args.train_iterations = 30000\n","\n","\n","Dbs = [1, 10, 100, 1000, 10000, 54000]\n","images = np.zeros((6, 28, 28))\n","\n","\n","for iter in range(0, 10):\n","    print(\"|--------- Iteration {} ---------|\".format(iter))\n","    for Db in Dbs:\n","        print(\"|--------- |D^b| {} ---------|\".format(Db))\n","\n","        # Set arguments\n","        args.logdir = './BatchSizes1/iter' + str(iter) + 'Db' + str(Db)\n","        args.path_assess = args.logdir + '/assess'\n","        args.path_model = args.logdir + '/model'\n","        args.path_summary = args.logdir + '/summary'\n","\n","        # Dataset\n","        dataset = Dataset(**vars(args))\n","\n","        # Reset the default graph and set a graph-level seed\n","        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","        tf.reset_default_graph()\n","        tf.set_random_seed(iter)\n","\n","        # Model\n","        model = Model(num_classes=dataset.num_classes, **vars(args))\n","        model.construct_model()\n","\n","        # Session\n","        sess = tf.InteractiveSession()\n","        tf.global_variables_initializer().run()\n","        tf.local_variables_initializer().run()\n","\n","        # Prune\n","        print('|========= START PRUNING =========|')\n","        t_start = time.time()\n","        batch = dataset.get_next_batch('train', Db)\n","        feed_dict = {}\n","        feed_dict.update({model.inputs[key]: batch[key] for key in ['input', 'label']})\n","        feed_dict.update({model.compress: True, model.is_train: False, model.pruned: False})\n","        result = sess.run([model.outputs, model.sparsity, model.mask['w1']], feed_dict)\n","        print('Pruning: {:.3f} global sparsity (t:{:.1f})'.format(result[1], time.time() - t_start))\n","\n","        # Average and reshape the indicator matrix c_1\n","        w1_mask = np.array(result[-1])\n","        images[Dbs.index(Db)] = np.reshape(np.mean(w1_mask, axis=1), (28, 28))\n","\n","        # Train and test\n","        train.train(args, model, sess, dataset)\n","        test.test(args, model, sess, dataset)\n","\n","        sess.close()"],"metadata":{"id":"G1HlBps4r3Vs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot and download the visualizations of pruned parameters\n","from google.colab import files\n","\n","for i in range(0, 6):\n","    fig_name = args.datasource + str(Dbs[i]) + '.png'\n","    fig = plt.imshow(images[i] * 255, cmap='gray')\n","    plt.axis('off')\n","    fig.axes.get_xaxis().set_visible(False)\n","    fig.axes.get_yaxis().set_visible(False)\n","    plt.savefig(fig_name, bbox_inches='tight', pad_inches = 0)\n","    files.download(fig_name) "],"metadata":{"id":"raIkuDnUy_aD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Batch Sizes for Pruning and Training"],"metadata":{"id":"m5K0RjlP3lw0"}},{"cell_type":"code","source":["import os\n","import sys\n","import argparse\n","import tensorflow as tf\n","\n","from dataset import Dataset\n","from model import Model\n","import prune\n","import train\n","import test\n","\n","\n","# Set arguments\n","args = Args()\n","args.arch = 'lenet300'\n","args.datasource = 'mnist' # mnist or kmnist\n","args.decay_boundaries = [5000, 10000, 15000, 20000, 25000]\n","args.decay_values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n","args.target_sparsity = 0.9 # 0.1, 0.5, 0.9\n","args.train_iterations = 30000\n","\n","\n","batch_sizes = [50, 100, 500, 1000]\n","\n","\n","for batch_size in batch_sizes:\n","    train_iterations = int(150000 / (batch_size / 100))\n","    decay_boundaries = []\n","    for time in range(1, 6):\n","        decay_boundaries.append(int(train_iterations / 6 * time))\n","\n","    for iter in range(0, 5):\n","        print(\"|--------- Iteration {} ---------|\".format(iter))\n","\n","        # Set arguments\n","        args.batch_size = batch_size\n","        args.logdir = './BatchSizes2/batch' + str(batch_size) + 'iter' + str(iter)\n","        args.path_assess = args.logdir + '/assess'\n","        args.path_model = args.logdir + '/model'\n","        args.path_summary = args.logdir + '/summary'\n","\n","        # Dataset\n","        dataset = Dataset(**vars(args))\n","\n","        # Reset the default graph and set a graph-level seed\n","        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","        tf.reset_default_graph()\n","        tf.set_random_seed(iter)\n","\n","        # Model\n","        model = Model(num_classes=dataset.num_classes, **vars(args))\n","        model.construct_model()\n","\n","        # Session\n","        sess = tf.InteractiveSession()\n","        tf.global_variables_initializer().run()\n","        tf.local_variables_initializer().run()\n","\n","        # Prune\n","        prune.prune(args, model, sess, dataset)\n","\n","        # Train and test\n","        train.train(args, model, sess, dataset)\n","        test.test(args, model, sess, dataset)\n","\n","        sess.close()"],"metadata":{"id":"bHyoLgmlryyE"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"run.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}